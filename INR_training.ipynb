{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "import torch\n",
    "\n",
    "gpu = torch.cuda.is_available()\n",
    "device = \"cuda\" if gpu else \"cpu\"\n",
    "tdevice = torch.device(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data with some normalisation\n",
    "As the grid is fixed, we normalise X, Y, Z, T\n",
    "\n",
    "T might be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from inr_src import return_dataset\n",
    "import inr_src as inr\n",
    "path = \"./data/test_data.npy\"\n",
    "ds, ds_test, nv, nv_y = inr.return_dataset(path, normalise_targets=False, gpu=False)\n",
    "\n",
    "print(\"##########\\n X, Y, T\")\n",
    "print(ds.samples)\n",
    "print(\"##########\\n Z\")\n",
    "print(ds.targets)\n",
    "print(\"##########\\n Shape\")\n",
    "print(ds.targets.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implicit Neural Representation\n",
    "\n",
    "Currently, different models are available: RFF or SIREN. I could also implement WIRES (based on wavelet functions).\n",
    "For these models, different archectures are available: \n",
    "\n",
    "The loss we are minimizing is the: $$\\mathcal{L} = ||f(x, y, t)  - z ||_2 + \\lambda_1 ||f(x, y, t)  - z ||_1 + \\lambda_{xy}||\\frac{\\mathrm{d} f}{\\mathrm{d} xy}||_2 + \\lambda_{t}||\\frac{\\mathrm{d} f}{ \\mathrm{d} t}||_2 $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from scipy import interpolate\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./data/test_data.npy\"\n",
    "\n",
    "opt = inr.AttrDict()\n",
    "opt.path = path\n",
    "opt.gpu = gpu\n",
    "opt.model_name = \"wires\" # or siren or wires or RFF\n",
    "opt.name = \"wires_notebook_unnormalised\"\n",
    "opt.normalise_targets = False\n",
    "model_hp = inr.AttrDict()\n",
    "\n",
    "opt.fourier = opt.model_name == \"RFF\"\n",
    "opt.siren = opt.model_name == \"siren\"\n",
    "opt.wires = opt.model_name == \"wires\"\n",
    "model_hp.fourier = opt.model_name == \"RFF\"\n",
    "model_hp.siren = opt.model_name == \"siren\"\n",
    "model_hp.wires = opt.model_name == \"wires\"\n",
    "\n",
    "\n",
    "model_hp.verbose = True\n",
    "model_hp.epochs = 10\n",
    "\n",
    "model_hp.bs = 2**16\n",
    "model_hp.scale = 5\n",
    "model_hp.lr = 1e-3\n",
    "model_hp.output_size = 1\n",
    "\n",
    "if opt.siren or opt.wires:\n",
    "    print(f\"Using {opt.model_name}\")\n",
    "    model_hp.architecture = opt.model_name\n",
    "    model_hp.hidden_num = 3\n",
    "    model_hp.hidden_dim = 256\n",
    "    model_hp.do_skip = True\n",
    "    if opt.wires:\n",
    "        model_hp.width_gaussian = 10.0\n",
    "else:\n",
    "    model_hp.mapping_size = 512\n",
    "    model_hp.architecture = \"skip-5\"  # \"Vlarge\"\n",
    "    model_hp.activation = \"tanh\"\n",
    "\n",
    "model_hp.lambda_l1 = 0.0\n",
    "model_hp.lambda_t = 0.0\n",
    "model_hp.lambda_xy = 0.0\n",
    "\n",
    "model, model_hp = inr.train(opt, model_hp, gpu=opt.gpu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or if you prefer to load the model\n",
    "opt = inr.AttrDict()\n",
    "opt.name = \"wires_notebook_unnormalised__test546\"\n",
    "## From saved\n",
    "npz = np.load(f\"meta/{opt.name}.npz\")\n",
    "\n",
    "\n",
    "weights = f\"meta/{opt.name}.pth\"\n",
    "\n",
    "model_hp = inr.AttrDict(npz)\n",
    "\n",
    "# model_hp.hidden_dim = model_hp.siren_hidden_dim\n",
    "# model_hp.hidden_num = model_hp.siren_hidden_num\n",
    "# model_hp.do_skip = model_hp.siren_skip\n",
    "model_hp.normalise_targets = False\n",
    "model_hp = inr.util_train.clean_hp(model_hp)\n",
    "\n",
    "\n",
    "model = inr.ReturnModel(\n",
    "    model_hp.input_size,\n",
    "    output_size=model_hp.output_size,\n",
    "    arch=model_hp.architecture,\n",
    "    args=model_hp,\n",
    ")\n",
    "print(f\"loading weight: {weights}\")\n",
    "model.load_state_dict(torch.load(weights, map_location=tdevice))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some data visualisation\n",
    "## Some real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation the data set by tempoarl averaging of each month\n",
    "pc = np.load(\"data/test_data.npy\")\n",
    "q33 = np.quantile(pc[:,3], 0.33)\n",
    "q66 = np.quantile(pc[:,3], 0.66)\n",
    "pc_t0 = pc[pc[:,3] < q33]\n",
    "pc_t1 = pc[(pc[:,3] >= q33) & (pc[:,3] < q66)]\n",
    "pc_t2 = pc[(pc[:,3] >= q66)]\n",
    "\n",
    "for spc, title in [(pc_t0, \"Jan.\"), (pc_t1, \"Feb.\"), (pc_t2, \"Mar.\")]:\n",
    "\n",
    "    idx = np.random.choice(np.arange(spc.shape[0]), replace=False, size=int(1e5))\n",
    "    fig = px.scatter_3d(x=spc[idx,0], y=spc[idx,1], z=spc[idx,2],\n",
    "                color=spc[idx,3], width=600, height=600)\n",
    "    fig.update_layout(title=title, legend_title_text=\"Time\")\n",
    "    fig.update_traces(marker_size=1)\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spc = pc_t0\n",
    "idx = np.random.choice(np.arange(spc.shape[0]), replace=False, size=int(1e5))\n",
    "fig = px.scatter(x=spc[idx,0], y=spc[idx,1], color=spc[idx,2], width=600, height=600, color_continuous_scale=px.colors.sequential.Viridis)\n",
    "            #color=spc[idx,3], width=1000, height=1000)\n",
    "fig.update_layout(\n",
    "    title=dict(text=\"January\", font=dict(size=50), automargin=True, yref='paper'),\n",
    "    font_size=40,\n",
    "    xaxis_title=\"Latitude\",\n",
    "    yaxis_title=\"Longitude\",\n",
    "    legend_title=\"Legend Title\",\n",
    "    coloraxis_colorbar=dict(\n",
    "        title=\"Height (m)\",\n",
    "    ),\n",
    ")\n",
    "fig.update_traces(marker_size=3)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x=spc[idx,0], y=spc[idx,1], color=spc[idx,3], \n",
    "                 width=600, height=600, \n",
    "                 color_continuous_scale=[(0, \"red\"), (0.5, \"green\"), (1, \"blue\")], )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=dict(text=\"\", font=dict(size=50), automargin=True, yref='paper'),\n",
    "    font_size=40,\n",
    "    xaxis_title=\"Latitude\",\n",
    "    yaxis_title=\"Longitude\",\n",
    "    legend_title=\"Legend Title\",\n",
    "    coloraxis_colorbar=dict(\n",
    "        title=\"\",\n",
    "    ),\n",
    ")\n",
    "fig.update_coloraxes(colorbar_tickvals=[np.quantile(spc[idx,3], perc) for perc in [0.04,0.2,0.4,0.6,0.8,0.96]], colorbar_ticktext=[r't<sub>{}</sub>'.format(i) for i in range(6)],)\n",
    "fig.update_traces(marker_size=3)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def interpolate_missing_pixels(\n",
    "        image: np.ndarray,\n",
    "        mask: np.ndarray,\n",
    "        method: str = 'nearest',\n",
    "        fill_value: int = 0\n",
    "):\n",
    "    \"\"\"\n",
    "    :param image: a 2D image\n",
    "    :param mask: a 2D boolean image, True indicates missing values\n",
    "    :param method: interpolation method, one of\n",
    "        'nearest', 'linear', 'cubic'.\n",
    "    :param fill_value: which value to use for filling up data outside the\n",
    "        convex hull of known pixel values.\n",
    "        Default is 0, Has no effect for 'nearest'.\n",
    "    :return: the image with missing values interpolated\n",
    "    \"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    xx, yy = np.meshgrid(np.arange(w), np.arange(h))\n",
    "\n",
    "    known_x = xx[~mask]\n",
    "    known_y = yy[~mask]\n",
    "    known_v = image[~mask]\n",
    "    missing_x = xx[mask]\n",
    "    missing_y = yy[mask]\n",
    "\n",
    "    interp_values = interpolate.griddata(\n",
    "        (known_x, known_y), known_v, (missing_x, missing_y),\n",
    "        method=method, fill_value=fill_value\n",
    "    )\n",
    "\n",
    "    interp_image = image.copy()\n",
    "    interp_image[missing_y, missing_x] = interp_values\n",
    "\n",
    "    return interp_image\n",
    "\n",
    "df = pd.DataFrame(spc[idx])\n",
    "df.columns = [\"x\", \"y\", \"z\", \"Time\"]\n",
    "df['ybin'] = pd.cut(df.y, [t for t in np.arange(df.y.min(), df.y.max()+0.1, 0.1)])\n",
    "df['xbin'] = pd.cut(df.x, [t for t in np.arange(df.x.min(), df.x.max()+0.01, 0.01)])\n",
    "pvtdf = df.pivot_table(index='xbin', columns=['ybin'], values='z',\n",
    "                    aggfunc=('mean'))\n",
    "z = pvtdf.values\n",
    "mask = np.ma.masked_invalid(z)\n",
    "test = interpolate_missing_pixels(z, mask.mask, method='linear')\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Contour(x=spc[idx,0],y=spc[idx,1],z=spc[idx,2],line_smoothing=1.3))\n",
    "fig.update_layout(autosize=False)\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trained model predicts Z from XYT of the trained points! (Should be good...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 50\n",
    "print(model(xytz_ds.samples[idx, :]))\n",
    "print(xytz_ds.samples[idx, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xytz_ds = inr.XYTZ(\n",
    "        path,\n",
    "        train_fold=False,\n",
    "        train_fraction=0.0,\n",
    "        seed=42,\n",
    "        pred_type=\"pc\",\n",
    "        nv=tuple(model_hp.nv),\n",
    "        nv_targets=tuple(model_hp.nv_target),\n",
    "        normalise_targets=model_hp.normalise_targets,\n",
    "        gpu=gpu\n",
    "    )\n",
    "\n",
    "prediction = inr.predict_loop(xytz_ds, 2048, model, device=device)\n",
    "\n",
    "mse_norm = mean_squared_error(xytz_ds.targets, prediction)\n",
    "mae_norm = mean_absolute_error(xytz_ds.targets, prediction)\n",
    "\n",
    "model_hp.nv_target = np.array(model_hp.nv_target)\n",
    "model_hp.nv = np.array(model_hp.nv)\n",
    "\n",
    "norm_target = xytz_ds.targets * model_hp.nv_target[0,1] + model_hp.nv_target[0,0]\n",
    "norm_pred = prediction * model_hp.nv_target[0,1] + model_hp.nv_target[0,0]\n",
    "mse_unnorm = mean_squared_error(norm_target, norm_pred)\n",
    "mae_unnorm = mean_absolute_error(norm_target, norm_pred)\n",
    "\n",
    "print(f\"Normalised data -> MSE: {mse_norm:.5f} MAE: {mae_norm:.5f}\")\n",
    "print(f\"True Z values ->   MSE: {mse_unnorm:.3f} MAE: {mae_unnorm:.3f}\")\n",
    "\n",
    "q33 = np.quantile(xytz_ds.samples[:,2], 0.33)\n",
    "q66 = np.quantile(xytz_ds.samples[:,2], 0.66)\n",
    "\n",
    "idx_0 = xytz_ds.samples[:,2] < q33\n",
    "idx_1 = (xytz_ds.samples[:,2] >= q33) & (xytz_ds.samples[:,2] < q66)\n",
    "idx_2 = (xytz_ds.samples[:,2] >= q66)\n",
    "\n",
    "for idx, title in [(idx_0, \"Jan.\"), (idx_1, \"Feb.\"), (idx_2, \"Mar.\")]:\n",
    "    samples = xytz_ds.samples[idx] * model_hp.nv[:,1] + model_hp.nv[:,0]\n",
    "    pred = prediction[idx,0] * model_hp.nv_target[0,1] + model_hp.nv_target[0,0]\n",
    "\n",
    "    idx = np.random.choice(np.arange(samples.shape[0]), replace=False, size=int(1e5))\n",
    "    fig = px.scatter_3d(x=samples[idx,0], y=samples[idx,1], z=pred[idx],\n",
    "                color=samples[idx,2])\n",
    "    fig.update_layout(title=title, legend_title_text=\"Time\")\n",
    "    fig.update_traces(marker_size=2)\n",
    "    fig.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting along the grid with \"t = middle of the month\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute linear grid for plotting\n",
    "#Capture a month\n",
    "q16 = np.quantile(pc[:,3], 0.1666)\n",
    "q50 = np.quantile(pc[:,3], 0.50)\n",
    "q83 = np.quantile(pc[:,3], 0.8333)\n",
    "\n",
    "dataset16 = inr.XYTZ_grid(pc, q16, nv=model_hp.nv, step_grid=0.01, gpu=gpu)\n",
    "dataset50 = inr.XYTZ_grid(pc, q50, nv=model_hp.nv, step_grid=0.01, gpu=gpu)\n",
    "dataset83 = inr.XYTZ_grid(pc, q83, nv=model_hp.nv, step_grid=0.01, gpu=gpu)\n",
    "nv = np.array(model_hp.nv)\n",
    "nv_target = np.array(model_hp.nv_target)\n",
    "XYT16 = dataset16.samples * nv[:,1] + nv[:,0]\n",
    "device = \"cuda\" if gpu else \"cpu\"\n",
    "# Predict along the grid\n",
    "preds16 = inr.predict_loop(dataset16, 2048, model, device=device)\n",
    "preds50 = inr.predict_loop(dataset50, 2048, model, device=device)\n",
    "preds83 = inr.predict_loop(dataset83, 2048, model, device=device)\n",
    "\n",
    "for spred, title in [(preds16, \"Jan.\"), (preds50, \"Feb.\"), (preds83, \"Mar.\")]:\n",
    "    spred_z = spred * nv_target[0,1] + nv_target[0,0]\n",
    "    fig = px.scatter_3d(x=XYT16[:,0], y=XYT16[:,1], z=spred_z[:,0],\n",
    "                color=spred_z[:,0])\n",
    "    fig.update_layout(title=title, legend_title_text=\"Time\")\n",
    "    fig.update_traces(marker_size=2)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Contour(x=XYT16[:,0],y=XYT16[:,1],z=spred_z[:,0],line_smoothing=1.3, colorscale=px.colors.sequential.Viridis))\n",
    "fig.update_layout(width=600, height=1000)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating animated gif for error\n",
    "from data_src.plot_utils import f, plot_scatter\n",
    "\n",
    "xytz_ds = inr.XYTZ(\n",
    "        path,\n",
    "        train_fold=False,\n",
    "        train_fraction=0.0,\n",
    "        seed=42,\n",
    "        pred_type=\"pc\",\n",
    "        nv=model_hp.nv,\n",
    "        nv_targets=model_hp.nv_target,\n",
    "        gpu=gpu\n",
    "    )\n",
    "\n",
    "prediction_ds = inr.predict_loop(xytz_ds, 2048, model, device=device)\n",
    "prediction_ds = prediction_ds * model_hp.nv_target[0,1] + model_hp.nv_target[0,0]\n",
    "true_target = xytz_ds.targets * model_hp.nv_target[0,1] + model_hp.nv_target[0,0]\n",
    "mae_error = true_target - prediction_ds\n",
    "mse_error = np.sign(mae_error) * (mae_error)**2\n",
    "\n",
    "\n",
    "\n",
    "sub_samples, sub_target = xytz_ds.samples, xytz_ds.targets\n",
    "sub_samples = sub_samples * model_hp.nv[:,1] + model_hp.nv[:,0]\n",
    "sub_target = sub_target * model_hp.nv_target[0,1] + model_hp.nv_target[0,0]\n",
    "sub_pred = prediction_ds[:,0] * model_hp.nv_target[0,1] + model_hp.nv_target[0,0]\n",
    "\n",
    "\n",
    "xrange = [sub_samples[:,0].min(), sub_samples[:,0].max()]\n",
    "yrange = [sub_samples[:,1].min(), sub_samples[:,1].max()]\n",
    "\n",
    "unique_times = np.unique(sub_samples[:,-1].numpy())\n",
    "unique_times.sort()\n",
    "\n",
    "dataset_t = inr.XYTZ_grid(pc, 0, nv=model_hp.nv, step_grid=0.01, gpu=gpu)\n",
    "XYT_xy = dataset_t.samples * model_hp.nv[:,1] + model_hp.nv[:,0]\n",
    "\n",
    "folder = \"gif/wires\"\n",
    "\n",
    "for t in unique_times:#\n",
    "    dataset_t.samples[:,-1] = (t - model_hp.nv[-1,0]) / model_hp.nv[-1,1]\n",
    "    idx_t = np.where(sub_samples[:,-1] == t)[0]\n",
    "    sub_sample_t = sub_samples[idx_t]\n",
    "    sub_target_t = sub_target[idx_t]\n",
    "\n",
    "    prediction = inr.predict_loop(dataset_t, 2048, model, device=device)\n",
    "    prediction = prediction * model_hp.nv_target[0,1] + model_hp.nv_target[0,0]\n",
    "\n",
    "\n",
    "    plot_scatter(XYT_xy, prediction.numpy()[:,0], [100, 1700], t, f\"{folder}/surface/{f(t)}.png\", px.colors.sequential.Viridis, xrange=xrange, yrange=yrange, isoline=True)\n",
    "\n",
    "\n",
    "    plot_scatter(sub_sample_t, sub_target_t[:,0], [100, 1700], t, f\"{folder}/GT/{f(t)}.png\", color=px.colors.sequential.Viridis, xrange=xrange, yrange=yrange)\n",
    "\n",
    "    mae_max = np.abs(mae_error).numpy().max()\n",
    "    # we cap the error to 15 for MAE and 300 for MSE\n",
    "    plot_scatter(sub_sample_t, mae_error[idx_t,0], [-15, 15], t, f\"{folder}/error/{f(t)}.png\", color=px.colors.diverging.RdBu, xrange=xrange, yrange=yrange)\n",
    "\n",
    "    mse_max = np.abs(mse_error).numpy().max()\n",
    "    plot_scatter(sub_sample_t, mse_error[idx_t,0], [-300, 300], t, f\"{folder}/squared_error/{f(t)}.png\", color=px.colors.diverging.RdBu, xrange=xrange, yrange=yrange)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import plotly.figure_factory as ff\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "x = mae_error.numpy()[:,0]\n",
    "hist_data = [x]\n",
    "group_labels = ['distplot'] # name of the dataset\n",
    "\n",
    "fig = ff.create_distplot(hist_data, group_labels)\n",
    "fig.write_image(\"mae_dist.png\")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
